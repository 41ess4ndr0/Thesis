{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a244822-956f-40d0-98be-29d628f3f436",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/how-to-fine-tune-gpt-2-for-text-generation-ae2ea53bc272"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c47b5e5f-ed34-4da8-91f2-e41cc717c5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laudi\\anaconda3\\envs\\work\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b104415e-1f99-477f-a2b1-1d9ee6613b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_FIND = re.compile(r\".*repository.\")\n",
    "rep_path = re.search(PATH_FIND,os.getcwd()).group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "128a657e-bc5c-4735-974d-456bcf785f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14d748d1-7592-471f-b27d-82a5c86dc8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Prepare data\n",
    "df = pd.read_csv(rep_path+\"\\\\99 generated csv\\\\ch 2\\\\raw_all_component.csv \")\n",
    "# lyrics = pd.read_csv('lyrics-data.csv')\n",
    "# lyrics = lyrics[lyrics['Idiom']=='ENGLISH']\n",
    "\n",
    "#Only keep popular artists, with genre Rock/Pop and popularity high enough\n",
    "# artists = pd.read_csv('artists-data.csv')\n",
    "# artists = artists[(artists['Genre'].isin(['Rock'])) & (artists['Popularity']>5)]\n",
    "# df = lyrics.merge(artists[['Artist', 'Genre', 'Link']], left_on='ALink', right_on='Link', how='inner')\n",
    "# df = df.drop(columns=['ALink','SLink','Idiom','Link'])\n",
    "\n",
    "#Drop the songs with lyrics too long (after more than 1024 tokens, does not work)\n",
    "# df = df[df['Lyric'].apply(lambda x: len(x.split(' ')) < 350)]\n",
    "\n",
    "# #Create a very small test set to compare generated text with the reality\n",
    "test_set = df.sample(n = 200, random_state=1998)\n",
    "df = df.loc[~df.index.isin(test_set.index)]\n",
    "\n",
    "#Reset the indexes\n",
    "test_set = test_set.reset_index()\n",
    "df = df.reset_index()\n",
    "\n",
    "#For the test set only, keep last 20 words in a new column, then remove them from original column\n",
    "test_set['True_end_abstract'] = test_set['abstract'].str.split().str[-20:].apply(' '.join)\n",
    "test_set['Abstract'] = test_set['abstract'].str.split().str[:-20].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60678c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\laudi\\\\OneDrive\\\\Desktop\\\\Tesi_workspace\\\\repository\\\\3 preparation'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78b22bdd-79df-4443-aa17-cf0b3b93e73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laudi\\OneDrive\\Desktop\\Tesi_workspace\\repository\\3 preparation\\python training\n"
     ]
    }
   ],
   "source": [
    "# PATH_FIND = re.compile(\".*repository.\")\n",
    "# rep_path = re.search(PATH_FIND,os.getcwd()).group()\n",
    "os.chdir(r\"C:\\Users\\laudi\\OneDrive\\Desktop\\Tesi_workspace\\repository\\3 preparation\")\n",
    "\n",
    "if \"python training\" not in os.listdir():\n",
    "    os.makedirs(r\"C:\\Users\\laudi\\OneDrive\\Desktop\\Tesi_workspace\\repository\\3 preparation\\python training\")\n",
    "\n",
    "\n",
    "os.chdir(\"python training\")\n",
    "print(os.getcwd())\n",
    "# ### Prepare data\n",
    "# df = pd.read_csv(rep_path+\"\\\\99 generated csv\\\\ch 2\\\\raw_all_component.csv \")\n",
    "df = pd.read_csv(r\"C:\\Users\\laudi\\OneDrive\\Desktop\\Tesi_workspace\\repository\\99 generated csv\\ch 2\\raw_all_component.csv \")\n",
    "\n",
    "# #Create a very small test set to compare generated text with the reality\n",
    "isfirst = input(\"Is it the first iteration? [Y/N]\\n>>> \")\n",
    "while True:\n",
    "    if isfirst == \"Y\":\n",
    "        test_set = df.sample(n = 40, random_state=1998)\n",
    "        df = df.loc[~df.index.isin(test_set.index)]\n",
    "        test_set = test_set.reset_index()\n",
    "        df = df.reset_index()\n",
    "        if \"csv\" not in os.listdir():\n",
    "            os.makedirs(\"csv\")\n",
    "        test_set.to_csv(r\"..//csv//test_set_gpt2.csv\", index = False)\n",
    "        df.to_csv(r\"..//csv//training_set_gpt2.csv\", index = False)\n",
    "        break\n",
    "    elif isfirst == \"N\":\n",
    "        test_set = pd.read_csv(r\"csv\\test_set_gpt2.csv\")\n",
    "        df = pd.read_csv(r\"csv\\training_set_gpt2.csv\")\n",
    "        break\n",
    "    else:\n",
    "        isfirst = input(\"Is it the first iteration? [Y/N]\\n>>> \")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0c28b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter n 0\n"
     ]
    }
   ],
   "source": [
    "range_training = input(\"select your range[1,2,3,4,5,6,7,8,9,10]:\\n>>>\")\n",
    "if range_training == \"1\":\n",
    "    df = df.iloc[:1000, :] \n",
    "elif range_training == \"2\":\n",
    "    df = df.iloc[1000:2000, :]\n",
    "elif range_training == \"3\":\n",
    "    df = df.iloc[2000:3000, :]\n",
    "elif range_training == \"4\":\n",
    "    df = df.iloc[3000:4000, :]\n",
    "elif range_training == \"5\":\n",
    "    df = df.iloc[4000:5000, :]\n",
    "elif range_training == \"6\":\n",
    "    df = df.iloc[5000:6000, :]\n",
    "elif range_training == \"7\":\n",
    "    df = df.iloc[6000:7000, :]\n",
    "elif range_training == \"8\":\n",
    "    df = df.iloc[7000:8000, :]\n",
    "elif range_training == \"9\":\n",
    "    df = df.iloc[8000:9000, :]\n",
    "\n",
    "#Reset the indexes\n",
    "\n",
    "\n",
    "#For the test set only, keep last 20 words in a new column, then remove them from original column\n",
    "test_set['True_end_abstract'] = test_set['abstract'].str.split().str[-20:].apply(' '.join)\n",
    "#test_set['Abstract'] = test_set['abstract'].str.split().str[:-20].apply(' '.join)\n",
    "\n",
    "#### Useful class for Abstract\n",
    "\n",
    "class Abstract(Dataset):  \n",
    "    def __init__(self, control_code, truncate=False, gpt2_type=\"gpt2\", max_length=1024):\n",
    "\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
    "        self.abstract = []\n",
    "        c = 0 \n",
    "        for row in df['abstract']:\n",
    "            self.abstract.append(torch.tensor(\n",
    "                self.tokenizer.encode(f\"<|{control_code}|>{row[:max_length]}<|endoftext|>\")\n",
    "            ))      \n",
    "            if c%2000 == 0:\n",
    "                print(f\"iter n {c}\")\n",
    "            c+=1\n",
    "        if truncate:\n",
    "            self.abstract = self.abstract[:2000] #it was 20000\n",
    "        self.abstract_count = len(self.abstract)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.abstract_count\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.abstract[item]\n",
    "isfirst = input(\"Is it the first model? [Y/N]\\n>>> \")\n",
    "while True:\n",
    "    if isfirst == \"Y\":\n",
    "        dataset = Abstract(df['abstract'], truncate=False, gpt2_type=\"gpt2\")\n",
    "        #Get the tokenizer and model\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        break\n",
    "    elif isfirst == \"N\":\n",
    "        last = os.listdir(\"pretrained\")[-1]\n",
    "        dataset = Abstract(df['abstract'], truncate=False, gpt2_type=\"gpt2\")\n",
    "        #Get the tokenizer and model\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        model = GPT2LMHeadModel.from_pretrained(r\"pretrained\\{last_}\".format(last_=last))\n",
    "        break\n",
    "    else:\n",
    "        isfirst = input(\"Is it the first model? [Y/N]\\n>>> \")\n",
    "#get the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7192ba6a-abb4-4cee-b440-33b4b4ab36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Abstract(Dataset):  \n",
    "    def __init__(self, control_code, truncate=False, gpt2_type=\"gpt2\", max_length=1024):\n",
    "\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
    "        self.abstract = []\n",
    "        c = 0 \n",
    "        for row in df['abstract']:\n",
    "            self.abstract.append(torch.tensor(\n",
    "                self.tokenizer.encode(f\"<|{control_code}|>{row[:max_length]}<|endoftext|>\")\n",
    "            ))      \n",
    "            if c%1000 == 0:\n",
    "                print(f\"iter n {c}\")\n",
    "            c+=1\n",
    "        if truncate:\n",
    "            self.abstract = self.abstract[:20000]\n",
    "        self.abstract_count = len(self.abstract)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.abstract_count\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.abstract[item]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c38b335e-50a5-4041-a4af-61f19436d64b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter n 0\n",
      "iter n 1000\n",
      "iter n 2000\n",
      "iter n 3000\n",
      "iter n 4000\n",
      "iter n 5000\n",
      "iter n 6000\n",
      "iter n 7000\n",
      "iter n 8000\n",
      "iter n 9000\n",
      "iter n 10000\n",
      "iter n 11000\n",
      "iter n 12000\n",
      "iter n 13000\n",
      "iter n 14000\n",
      "iter n 15000\n",
      "iter n 16000\n",
      "iter n 17000\n",
      "iter n 18000\n",
      "iter n 19000\n",
      "iter n 20000\n",
      "iter n 21000\n",
      "iter n 22000\n",
      "iter n 23000\n",
      "iter n 24000\n",
      "iter n 25000\n",
      "iter n 26000\n",
      "iter n 27000\n",
      "iter n 28000\n",
      "iter n 29000\n",
      "iter n 30000\n",
      "iter n 31000\n",
      "iter n 32000\n",
      "iter n 33000\n",
      "iter n 34000\n",
      "iter n 35000\n",
      "iter n 36000\n",
      "iter n 37000\n",
      "iter n 38000\n",
      "iter n 39000\n",
      "iter n 40000\n",
      "iter n 41000\n",
      "iter n 42000\n",
      "iter n 43000\n",
      "iter n 44000\n",
      "iter n 45000\n",
      "iter n 46000\n",
      "iter n 47000\n",
      "iter n 48000\n",
      "iter n 49000\n"
     ]
    }
   ],
   "source": [
    "dataset = Abstract(df['abstract'], truncate=True, gpt2_type=\"gpt2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4491f10f-f050-4647-aff6-7b9f5816f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(r\"pretrained/pretrained-2023-01-22/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16285bbc-4aaf-4184-86bf-c5236c55af44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "#Accumulated batch size (since GPT2 is so big)\n",
    "def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n",
    "    if packed_tensor is None:\n",
    "        return new_tensor, True, None\n",
    "    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n",
    "        return packed_tensor, False, new_tensor\n",
    "    else:\n",
    "        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n",
    "        return packed_tensor, True, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d3da415-a145-419c-a4c3-12b403433160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataset, model, tokenizer,\n",
    "    batch_size=16, epochs=5, lr=2e-5,\n",
    "    max_seq_len=400, warmup_steps=200,\n",
    "    gpt2_type=\"gpt2\", output_dir=\".\", output_prefix=\"wreckgar\",\n",
    "    test_mode=False,save_model_on_epoch=False,save_model = False\n",
    "):\n",
    "    acc_steps = 100\n",
    "    device=torch.device(\"cuda\")\n",
    "    model = model.cuda()\n",
    "    model.train()\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    loss=0\n",
    "    accumulating_batch_count = 0\n",
    "    input_tensor = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(f\"Training epoch {epoch}\")\n",
    "        print(loss)\n",
    "        for idx, entry in tqdm(enumerate(train_dataloader)):\n",
    "            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)\n",
    "\n",
    "            if carry_on and idx != len(train_dataloader) - 1:\n",
    "                continue\n",
    "\n",
    "            input_tensor = input_tensor.to(device)\n",
    "            outputs = model(input_tensor, labels=input_tensor)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "\n",
    "            if (accumulating_batch_count % batch_size) == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                model.zero_grad()\n",
    "\n",
    "            accumulating_batch_count += 1\n",
    "            input_tensor = None\n",
    "        if save_model_on_epoch:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(output_dir, f\"{output_prefix}-{epoch}.pt\"),\n",
    "            )\n",
    "        if save_model:\n",
    "            model.save_pretrained(f\"pretrained//pretrained-{str(dt.datetime.now())[:10]}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0ec86e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-01-24-h14-m41'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime as dt\n",
    "f\"{str(dt.datetime.now())[:10]}-h{str(dt.datetime.now())[11:13]}-m{str(dt.datetime.now())[14:16]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373fb32a-a85a-4885-914d-26006113b380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3118it [03:58, 13.44it/s]"
     ]
    }
   ],
   "source": [
    "model = train(dataset, model, tokenizer, batch_size=1, epochs=3, save_model_on_epoch = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0515a25c-c250-4890-8d05-cd70b233aa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    entry_count=10,\n",
    "    entry_length=30, #maximum number of words\n",
    "    top_p=0.8,\n",
    "    temperature=1.,\n",
    "):\n",
    "    model.eval()\n",
    "    generated_num = 0\n",
    "    generated_list = []\n",
    "\n",
    "    filter_value = -float(\"Inf\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for entry_idx in trange(entry_count):\n",
    "\n",
    "            entry_finished = False\n",
    "            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "\n",
    "            for i in range(entry_length):\n",
    "                outputs = model(generated, labels=generated)\n",
    "                loss, logits = outputs[:2]\n",
    "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                    ..., :-1\n",
    "                ].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = filter_value\n",
    "\n",
    "                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "                generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "                if next_token in tokenizer.encode(\"<|endoftext|>\"):\n",
    "                    entry_finished = True\n",
    "\n",
    "                if entry_finished:\n",
    "\n",
    "                    generated_num = generated_num + 1\n",
    "\n",
    "                    output_list = list(generated.squeeze().numpy())\n",
    "                    output_text = tokenizer.decode(output_list)\n",
    "                    generated_list.append(output_text)\n",
    "                    break\n",
    "            \n",
    "            if not entry_finished:\n",
    "                output_list = list(generated.squeeze().numpy())\n",
    "                output_text = f\"{tokenizer.decode(output_list)}<|endoftext|>\" \n",
    "                generated_list.append(output_text)\n",
    "                \n",
    "    return generated_list\n",
    "\n",
    "#Function to generate multiple sentences. Test data should be a dataframe\n",
    "def text_generation(test_data):\n",
    "    generated_abstract = []\n",
    "    for i in range(len(test_data)):\n",
    "        x = generate(model.to('cpu'), tokenizer, test_data['Abstract'][i], entry_count=1)\n",
    "        generated_abstract.append(x)\n",
    "    return generated_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b84460e-b57c-41bf-813e-d8083a91e2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be221a1a-5974-440b-a9f2-18200e703354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:17<00:00, 17.44s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:16<00:00, 16.62s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:16<00:00, 16.21s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:19<00:00, 19.82s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:22<00:00, 22.14s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:19<00:00, 19.67s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:20<00:00, 20.82s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:13<00:00, 13.88s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:13<00:00, 13.67s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:19<00:00, 19.72s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:13<00:00, 13.92s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:19<00:00, 19.11s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:11<00:00, 11.68s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:15<00:00, 15.19s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:11<00:00, 11.62s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:13<00:00, 13.55s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:21<00:00, 21.46s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.39s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:18<00:00, 18.81s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:14<00:00, 14.66s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:18<00:00, 18.45s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:20<00:00, 20.37s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:20<00:00, 20.20s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.50s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:19<00:00, 19.14s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:17<00:00, 17.74s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:16<00:00, 16.23s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:17<00:00, 17.56s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:19<00:00, 19.55s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:22<00:00, 22.89s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:15<00:00, 15.56s/it]\n",
      "  0%|                                                                                            | 0/1 [00:03<?, ?it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Run the functions to generate the lyrics\n",
    "generated_abstarct = text_generation(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa994b46-021e-4005-a1ba-e33eab692e06",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generated_abstract' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Loop to keep only generated text and add it as a new column in the dataframe\u001b[39;00m\n\u001b[0;32m      2\u001b[0m my_generations\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mgenerated_abstract\u001b[49m)):\n\u001b[0;32m      5\u001b[0m     a \u001b[38;5;241m=\u001b[39m test_set[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLyric\u001b[39m\u001b[38;5;124m'\u001b[39m][i]\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m30\u001b[39m:] \u001b[38;5;66;03m#Get the matching string we want (30 words)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(a)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'generated_abstract' is not defined"
     ]
    }
   ],
   "source": [
    "#Loop to keep only generated text and add it as a new column in the dataframe\n",
    "my_generations=[]\n",
    "\n",
    "for i in range(len(generated_abstract)):\n",
    "    a = test_set['Abstract'][i].split()[-30:] #Get the matching string we want (30 words)\n",
    "    b = ' '.join(a)\n",
    "    c = ' '.join(generated_abstract[i]) #Get all that comes after the matching string\n",
    "    my_generations.append(c.split(b)[-1])\n",
    "\n",
    "test_set['Generated_abstract'] = my_generations\n",
    "\n",
    "\n",
    "#Finish the sentences when there is a point, remove after that\n",
    "final=[]\n",
    "\n",
    "for i in range(len(test_set)):\n",
    "    to_remove = test_set['Generated_abstract'][i].split('.')[-1]\n",
    "    final.append(test_set['Generated_abstract'][i].replace(to_remove,''))\n",
    "\n",
    "test_set['Generated_abstract'] = final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750a08bd-31ef-4be3-a0e2-38cc89dbba2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b44035c-835b-44c9-8c4c-c24e31b55f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Using BLEU score to compare the real sentences with the generated ones\n",
    "import statistics\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "scores=[]\n",
    "\n",
    "for i in range(len(test_set)):\n",
    "    reference = test_set['True_end_lyrics'][i]\n",
    "    candidate = test_set['Generated_lyrics'][i]\n",
    "    scores.append(sentence_bleu(reference, candidate))\n",
    "\n",
    "statistics.mean(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 24 2022, 14:07:00) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "3e073b816adbb62516ef6a1fb0a012a532d644a48bac22b8bcc94c77b6ed60c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
